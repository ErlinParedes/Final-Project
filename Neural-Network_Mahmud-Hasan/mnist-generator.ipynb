{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b91a390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], Batch [0/938], D Loss: 1.3968, G Loss: 0.6935\n",
      "Epoch [0/10], Batch [900/938], D Loss: 1.1721, G Loss: 0.7949\n",
      "Epoch [1/10], Batch [0/938], D Loss: 1.2235, G Loss: 0.7505\n",
      "Epoch [1/10], Batch [900/938], D Loss: 0.9341, G Loss: 1.0426\n",
      "Epoch [2/10], Batch [0/938], D Loss: 1.1941, G Loss: 0.7952\n",
      "Epoch [2/10], Batch [900/938], D Loss: 1.6778, G Loss: 0.5687\n",
      "Epoch [3/10], Batch [0/938], D Loss: 1.5920, G Loss: 0.6017\n",
      "Epoch [3/10], Batch [900/938], D Loss: 1.2262, G Loss: 0.8026\n",
      "Epoch [4/10], Batch [0/938], D Loss: 1.3127, G Loss: 0.7521\n",
      "Epoch [4/10], Batch [900/938], D Loss: 1.5375, G Loss: 0.6345\n",
      "Epoch [5/10], Batch [0/938], D Loss: 1.2383, G Loss: 0.8614\n",
      "Epoch [5/10], Batch [900/938], D Loss: 1.4934, G Loss: 0.6959\n",
      "Epoch [6/10], Batch [0/938], D Loss: 1.5761, G Loss: 0.6780\n",
      "Epoch [6/10], Batch [900/938], D Loss: 1.4038, G Loss: 0.7608\n",
      "Epoch [7/10], Batch [0/938], D Loss: 1.3838, G Loss: 0.7631\n",
      "Epoch [7/10], Batch [900/938], D Loss: 0.8980, G Loss: 1.2652\n",
      "Epoch [8/10], Batch [0/938], D Loss: 1.0047, G Loss: 1.0440\n",
      "Epoch [8/10], Batch [900/938], D Loss: 1.2536, G Loss: 0.8547\n",
      "Epoch [9/10], Batch [0/938], D Loss: 1.4218, G Loss: 0.7786\n",
      "Epoch [9/10], Batch [900/938], D Loss: 1.1889, G Loss: 0.9039\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size + 10, hidden_size),  # Extra 10 for the one-hot encoded labels\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x = torch.cat([x, labels], 1)  # Concatenate the labels to the noise vector\n",
    "        return self.fc(x)\n",
    "\n",
    "# Discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size + 10, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Sigmoid()  # Use sigmoid for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x = torch.cat([x, labels], 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "z_size = 100  # Size of the random noise vector\n",
    "hidden_size = 128\n",
    "\n",
    "# Load dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "# if the MNIST data is already downloaded\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=False, transform=transform)\n",
    "\n",
    "if not train_dataset:\n",
    "    # If not, download the data\n",
    "    train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize networks and optimizers\n",
    "generator = Generator(z_size, hidden_size, 28*28)\n",
    "discriminator = Discriminator(28*28, hidden_size, 1)\n",
    "\n",
    "# Use proper weight initialization\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch, (real_images, real_labels) in enumerate(train_loader):\n",
    "        actual_batch_size = real_images.size(0)\n",
    "        real_labels_one_hot = torch.zeros(actual_batch_size, 10)\n",
    "        real_labels_one_hot[torch.arange(actual_batch_size), real_labels] = 1\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        real_images = real_images.view(-1, 28*28)\n",
    "        real_labels = real_labels_one_hot\n",
    "\n",
    "        # Soft labels for real samples (label smoothing)\n",
    "        real_labels_smooth = 0.9 * torch.ones(actual_batch_size, 1)\n",
    "        fake_labels_smooth = 0.1 * torch.ones(actual_batch_size, 1)\n",
    "\n",
    "        # Forward pass real batch through discriminator\n",
    "        output_real = discriminator(real_images, real_labels_one_hot)\n",
    "        loss_real = nn.BCELoss()(output_real, real_labels_smooth)\n",
    "\n",
    "        # Generate fake images\n",
    "        noise = Variable(torch.randn(actual_batch_size, z_size))\n",
    "        fake_images = generator(noise, real_labels_one_hot)\n",
    "\n",
    "        # Forward pass fake batch through discriminator\n",
    "        output_fake = discriminator(fake_images.detach(), real_labels_one_hot)\n",
    "        loss_fake = nn.BCELoss()(output_fake, fake_labels_smooth)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_d = loss_real + loss_fake\n",
    "        loss_d.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Forward pass fake batch through discriminator again\n",
    "        output_fake = discriminator(fake_images, real_labels_one_hot)\n",
    "        loss_g = nn.BCELoss()(output_fake, real_labels_smooth)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_g.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Print loss\n",
    "        if batch % 900 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Batch [{batch}/{len(train_loader)}], '\n",
    "                  f'D Loss: {loss_d.item():.4f}, G Loss: {loss_g.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deabf28-285d-44bd-93a8-50bb664609da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
